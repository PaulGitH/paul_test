{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSS609 : Text Analytics and Applications\n",
    "\n",
    "## Lab: Week 3\n",
    "\n",
    "### Agenda \n",
    "\n",
    "- Download NLTK Data\n",
    "\n",
    "- Install gensim\n",
    "\n",
    "- Load text data\n",
    "   - To be able to load your own text collections.\n",
    "   - Explore the files in your corpus\n",
    "   - Freq distribution of words\n",
    "\n",
    "- NLP Concepts - Basics\n",
    "    - To be able to perform basic text processing operations (tokenization, lemmatization, stop words removal and punctuation removal) using Python and Gensim.\n",
    "\n",
    "- NLP Concepts - Advanced \n",
    "    - POS Tagging\n",
    "    - Syntax parse tree (Visualization) \n",
    "     \n",
    "- Optional Labs\n",
    "    - Syntax Parse Tree Generation (Advanced)\n",
    "    \n",
    "- Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NLTK\n",
    "### Some Basic Usage of NLTK\n",
    "\n",
    "NLTK is a natural language toolkit for building programs in Python that work with natural language text. We will use NLTK for text pre-processing and some other tasks this term.\n",
    "When you installed Anaconda, NLTK should have already been installed.\n",
    "However, for the exercises below, you will need to download some data.\n",
    "\n",
    "Before we download the data, please create a directory called __nltk_data__ and leave this directory empty.\n",
    "- Windows: In 'C:\\'. \n",
    "![windows](images/nltk-folder-win.png)\n",
    "\n",
    "- Mac: In your home folder ~\n",
    "![mac](images/nltk-folder.png)\n",
    "\n",
    "\n",
    "Now run the code below. Note that the code may take some to run, and you'll need to wait until you don't see __In [\\*]__ anymore on the left of the cell below. The code does not generate any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us download some data using the following code. When you run the code `nltk.download()` below, you'll see a pop-up window as shown below. Select \"book\" and start downloading. It will take some time to download the data. After the download is finished, you <b>close</b> the \"NLTK Downloader\" window.\n",
    "![mac](images/nltk-download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please pay Attention: A window should open after you enter the following command. If you can't see, \n",
    "# it might be behind the browser window. Choose book, check the directories and then click \n",
    "# download. Also, be patient. This is slow.\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your Own Text - SampleData\n",
    "\n",
    "The first part of this lab is to show you how to load your own text collections by opening the file in Python and loading it into the NLTK model.\n",
    "\n",
    "We typically refer to a digitalized collection of documents as a\n",
    "*corpus*. A corpus contains a set of documents. While many real world\n",
    "documents are in the format of Microsoft Word or PDF, when we process\n",
    "and analyze documents, they are usually converted into plain text files.\n",
    "Here we assume that the corpus we plan to analyze contains only plain\n",
    "text files.\n",
    "\n",
    "First, create one `.txt` file for each document in your collection. For\n",
    "example, if you have 100 documents, you can name them `1.txt`, `2.txt`,\n",
    "$\\ldots$, `100.txt`. You are free to choose any document name as long as\n",
    "each document has a unique name. Place all the `.txt` files in a\n",
    "directory of your choice. \n",
    "\n",
    "Next, you would like to load these files into the NLTK model by reading them. This is because there are in built tool in NLTK that can aid in the preprocessing of the documents. Let us start with a simple model  to deal with plain text files that do not have annotations such as HTML tags.\n",
    "\n",
    "The following code shows how you can load plain text files using NLTK. In\n",
    "the example below, it is assume that there are two files named `haze.txt` and `mrt.txt`inside the directory `data/SampleText`, where `data` should be placed in the current directory, i.e., where this Jupyter notebook is placed.\n",
    "(Note that the `data` directory with the two text files inside should have been downloaded together with this Jupyter notebook.)\n",
    "If you have placed the data in a different directory, you can modify the code below to correspond to the correct directory where your files are.\n",
    "We also encourage you to create different folders for different labs to avoid confusion.\n",
    "\n",
    "For those of you new to Python, the lines starting with `#` are *comments*, which explain what the code does but cannot be executed.\n",
    "\n",
    "For documentation (e.g. on `PlaintextCorpusReader`), go to https://www.kite.com/python/docs/ and search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of Gensim\n",
    "\n",
    "For this lab, you will use Gensim, a Python library that provides some\n",
    "built-in functions for easily converting documents to vectors and\n",
    "computing cosine similarities. Although you can always write your own\n",
    "code to do this, it is much easier for beginners to make use of existing\n",
    "libraries. It is also very common for programmers to re-use libraries\n",
    "developed by other programmers.\n",
    "\n",
    "To install Gensim under Anaconda, open your Anaconda Prompt window and type the following command inside the Anaconda Prompt window::\n",
    "\n",
    "`conda install -c anaconda gensim`\n",
    "\n",
    "When you're asked whether you want to proceed with the installation as shown below, please answer `y`.\n",
    "\n",
    "`Proceed ([y]/n)? y`\n",
    "\n",
    "\n",
    "The installation process may take some time so please be patient.\n",
    "\n",
    "After Gensim is installed, try the following code to see if it can be imported. You may get a warning message that says `'warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")'`. You can ignore the warning message.\n",
    "\n",
    "This lab is based on the tutorial - https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "\n",
    "Check if the installation is sucessful by importing gensim. A warning can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start loading in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 2\n",
      "Number of words: 258\n",
      "Names of files: ['haze.txt', 'mrt.txt']\n"
     ]
    }
   ],
   "source": [
    "# import nltk # Already done in the first cell\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Define the file directory\n",
    "file_directory = 'data/SampleText/'\n",
    "\n",
    "filename_pattern = '.+\\.txt'\n",
    "\n",
    "my_corpus = PlaintextCorpusReader(file_directory, filename_pattern)\n",
    "# print(my_corpus.fileids())\n",
    "\n",
    "# Number of documents in the collection.\n",
    "print(\"Number of files:\", len(my_corpus.fileids()))\n",
    "\n",
    "# Total number of words in the collection.\n",
    "print(\"Number of words:\", len(my_corpus.words()))\n",
    "\n",
    "# Print the filenames in the corpus\n",
    "print(\"Names of files:\", my_corpus.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can take a look at the file named haze.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from Python method:\n",
      " Singapore can expect more rain and less haze in the coming weeks with the south-west monsoon season transitioning into inter-monsoon conditions.\n",
      "\n",
      "The inter-monsoon season typically lasts from October to November and the weather during the period is characterised by more rainfall and light and variable winds.\n",
      "\n",
      "The Meteorological Service Singapore said on Monday in an advisory that this transition signals the end of traditional dry season in the region, and the likelihood of transboundary haze affecting Singapore for the rest of the year will be low.\n",
      "\n",
      "This is because the increased rainfall will help alleviate the hotspot and haze situation in Sumatra and Kalimantan in Indonesia.\n",
      "\n",
      "Text from NLTK Corpus Reader:\n",
      " ['Singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', '-', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', '-', 'monsoon', 'conditions', '.', 'The', 'inter', '-', 'monsoon']\n"
     ]
    }
   ],
   "source": [
    "# Code to read a single file - Read the file that you want to look at eg haze.txt\n",
    "with open(file_directory + 'haze.txt', 'r') as file_to_read:\n",
    "    haze2 = file_to_read.read()\n",
    "    \n",
    "print(\"Text from Python method:\\n\", haze2)\n",
    "\n",
    "# Print some words in specific file\n",
    "haze = my_corpus.words('haze.txt')\n",
    "print(\"Text from NLTK Corpus Reader:\\n\", haze[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freq Distribution of words\n",
    "\n",
    "The function FreqDist() provided by NLTK. FreqDist() can be applied to any list in Python. We can now use FreqDist() on our own text as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq. Dist. from NLTK:\n",
      " [('the', 11), ('and', 7), ('in', 5), ('.', 4), ('Singapore', 3), ('haze', 3), ('-', 3), ('monsoon', 3), ('season', 3), ('of', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Word counts distribution for haze.txt\n",
    "fdist = nltk.FreqDist(haze)\n",
    "print(\"Freq. Dist. from NLTK:\\n\", fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the code above displays the most frequent 10 words\n",
    "inside the document `haze.txt`.\n",
    "\n",
    "What if you would like to get the words from *all* the files in\n",
    "`my_corpus`? You can simply use `my_corpus.words()` without specifying any document ID. Give it a try. \n",
    "\n",
    "#### The above list has punctuations and stopwords. Let's apply NLP concepts to process the data We shall use gensim API for text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Concepts - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have noticed that by using the `PlaintextCorpusReader`,\n",
    "tokenization is done while loading the files, that is, the original text\n",
    "is split into individual words and stored as a list of words in Python.\n",
    "\n",
    "We are using gensim now and this requires explicit tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Cast this into a list as the result would be a generator object otherwise\n",
    "haze2 = list(gensim.utils.tokenize(haze2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from Gensim:\n",
      " ['Singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', 'monsoon', 'conditions', 'The', 'inter', 'monsoon', 'season', 'typically', 'lasts', 'from', 'October', 'to', 'November', 'and', 'the', 'weather', 'during', 'the', 'period', 'is', 'characterised', 'by', 'more', 'rainfall', 'and', 'light', 'and', 'variable', 'winds', 'The', 'Meteorological', 'Service', 'Singapore', 'said', 'on', 'Monday', 'in', 'an', 'advisory', 'that', 'this', 'transition', 'signals', 'the', 'end', 'of', 'traditional', 'dry', 'season', 'in', 'the', 'region', 'and', 'the', 'likelihood', 'of', 'transboundary', 'haze', 'affecting', 'Singapore', 'for', 'the', 'rest', 'of', 'the', 'year', 'will', 'be', 'low', 'This', 'is', 'because', 'the', 'increased', 'rainfall', 'will', 'help', 'alleviate', 'the', 'hotspot', 'and', 'haze', 'situation', 'in', 'Sumatra', 'and', 'Kalimantan', 'in', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens from Gensim:\\n\", haze2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the tokenize function in Gensim already removes punctuation for you.\n",
    "\n",
    "### Changing Everything to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haze2_lower = [w.lower() for w in haze2]\n",
    "print(\"Lowercase tokens from Gensim:\\n\", haze2_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal\n",
    "\n",
    "Gensim also has a built-in stop word list for English that can come in\n",
    "handy when we need to remove stop words from a text collection. The\n",
    "following code shows how we remove all the stop words from the list\n",
    "`haze2_lower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in stop word list (note that this is a set rather than a list)\n",
    "stop_list = gensim.parsing.preprocessing.STOPWORDS\n",
    "\n",
    "haze2_stopremoved = [w for w in haze2_lower if w not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lowercase tokens with stop words removed (from Gensim):\\n\", haze2_stopremoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Gensim also has a built-in Porter stemmer we can use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "haze2_stemmed = [stemmer.stem(w) for w in haze2_stopremoved]\n",
    "print(\"Lowercase tokens with stop words removed and stemmed (from Gensim):\\n\", haze2_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the code above that using the Porter stemmer, “coming”\n",
    "is changed to “come,” “weeks” is changed to “week,” and “transitioning”\n",
    "is changed to “transit.” We can also see that after stemming, some words\n",
    "are no longer correct. For example, “singapore” is changed to\n",
    "“singapor,” “conditions” is changed to “condit,” and so on. Although for\n",
    "humans, these words no longer make sense, for computers, this is usually\n",
    "not a problem. As long as all occurrences of “singapore” are changed to\n",
    "“singapor” and all occurrences of “conditions” or “condition” are\n",
    "changed to “condit,” we can still perform many analysis tasks. For\n",
    "example, to search for relevant documents about “singapore,” after\n",
    "stemming, we just need to search for documents containing the word\n",
    "“singapor.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freq Distritbution of Words\n",
    "We use a function called Counter for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words2 = [token for token in haze2_stemmed]\n",
    "word_freq2 = Counter(words2)\n",
    "common_words2 = word_freq2.most_common(10)\n",
    "\n",
    "print(\"Freq. Dist (counter) from Gensim:\\n\", common_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Concepts - Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - POS Tagging\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset. Our emphasis in this lab is on exploiting tags, and tagging text automatically.\n",
    "Refer to Penn Tree bank for tags - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = word_tokenize(\"John ate the cake with spoon\")\n",
    "\n",
    "print(text)\n",
    "\n",
    "tagsText= nltk.pos_tag(text)\n",
    "print(tagsText);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    " Write the code to generate tags for the text \"After she ate the cake, Emma visited Tony in his room\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "text2 = \n",
    "tagsText2 =\n",
    "print(tagsText2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Syntactically Parsed Tree\n",
    "\n",
    "### NLTK parser - Parsing with context free grammar\n",
    "\n",
    "A parser processes input sentences according to the productions of a grammar, and builds one or more constituent structures that conform to the grammar. \n",
    "\n",
    "\n",
    "### A Simple Grammar\n",
    "1. We need to first define a context free grammar rules. we use pattern to define the rules. \n",
    "2. We then use the chunker to chunk the POS tagged sentence that uses the grammar pattern to generate the syntax parse tree.\n",
    "\n",
    "A grammar based chunk parser. chunk.RegexpParser uses a set of regular expression patterns to specify the behavior of the parser. The chunking of the text is encoded using a ChunkString, and each rule acts by modifying the chunking in the ChunkString. The rules are all implemented using regular expression matching and substitution.\n",
    "\n",
    "A grammar contains one or more clauses in the following form:\n",
    "Examples: \n",
    "\n",
    "1.    NP:\n",
    "       {<DT|JJ>}          # chunk determiners and adjectives\n",
    "2.    VP:\n",
    "        {VBD| VBD NP}     # Chunk verbs and noun phrases  \n",
    "  \n",
    "https://kite.com/python/docs/nltk.chunk.RegexpParser\n",
    "\n",
    "3. To convert into a better visuals, we can use the Tree class. Below is the code.\n",
    "\n",
    "After you execute the code, what do you observe in the output? Does the output show you the correct syntax tree? What should we do to improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import Tree\n",
    "\n",
    "pattern = \"\"\"NP: {<DT>?<JJ>*<NN>}\n",
    " VBD: {<VBD>}\n",
    " IN: {<IN>}\"\"\"\n",
    "\n",
    "# Code to call the Chunker to interpret the grammar and generate the syntax structure on the tagged text. \n",
    "NPChunker = nltk.RegexpParser(pattern) \n",
    "result = NPChunker.parse(tagsText)\n",
    "result.pprint()\n",
    "\n",
    "# To print the visual, as art\n",
    "parse = Tree.fromstring(str(result))\n",
    "parse.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "Write the code to draw the parse tree for the second sentence (`text2`) \"After she ate the cake, Emma visited Tony in his room\"  \n",
    "Use its tagged output `tagsText2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "result2 = \n",
    "result2.pprint()\n",
    "\n",
    "# To print the visual, as art\n",
    "parse2 = \n",
    "parse2.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Parse tree (Improve the tree)\n",
    "We notice that the syntaxt structure is incomplete. Add more clause and generate better syntax parsed tree\n",
    "\n",
    " Write the code to display the parse tree for the text \n",
    " 1. John ate the cake with spoon\n",
    " 2. After she ate the cake, Emma visited Tony in his room\n",
    " \n",
    " We will have various answers according to the grammar clauses you have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = \"\"\"NP: {<DT>?<JJ>*<NN>}\n",
    " IN: {<IN>}\n",
    " PP: {<IN>*<IN>?<NP>}\n",
    " VB: {<VBP>}\n",
    " VBD: {<VB>*<VB>?<NP>*<VB>?<NP>?<PP>}\n",
    " \"\"\"\n",
    "NPChunker = nltk.RegexpParser(pattern) \n",
    "\n",
    "result = NPChunker.parse(tagsText)\n",
    "result.pprint()\n",
    "\n",
    "parse = Tree.fromstring(str(result))\n",
    "parse.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4  - Stanford Parser (Totally Optional!)\n",
    "The tree may be still incomplete. This is because the grammar clauses are incomplete. Creating a complete list of grammar clauses  is the first solution to this problem. The second one to download a better parser and use it. Let's try Stanford CoreNLP.  \n",
    "\n",
    "Here are the steps:\n",
    "- Go to https://stanfordnlp.github.io/CoreNLP/ and download the latest version of CoreNLP\n",
    "- Unzip it and move it where Jupyter Notebook can access it. Example: `~/TAA/Labs/CoreNLP` or `C:\\TAA\\Labs\\CoreNLP`\n",
    "- Start a Core NLP Server using the downloaded files (easier than it sounds!)\n",
    "- Use the `CoreNLPParser` from NLTK to commnunicate with the server\n",
    "- Parse anything now:\n",
    "    - For constituency-parsing, use `CoreNLPParser`\n",
    "    - For dependency parsing, use `CoreNLPDependencyParser`\n",
    "\n",
    "Credit: Parts 4 and 5 of the lab are based on https://bbengfort.github.io/snippets/2018/06/22/corenlp-nltk-parses.html\n",
    "\n",
    "#### Notes\n",
    "- The folder where you store `CoreNLP` has to be visible to the Jupyter. In other words, you should be able to browse to it from the Jupyter NB file browser window.\n",
    "- You need Java, which is a pain to install in M1 MacBooks.\n",
    "- On Windows, you need GhostScript. Download it from https://www.ghostscript.com/download/gsdnld.html and install it using default setup.\n",
    "- You may have to specify the paths (on Windows again - maybe you should get a Mac!). The code is specified in the code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "import os\n",
    "import platform\n",
    "\n",
    "myOS = platform.system()\n",
    "\n",
    "if (myOS == \"Windows\"):\n",
    "    os.environ[\"PATH\"] += os.pathsep + r'C:\\Program Files\\gs\\gs9.52\\bin'\n",
    "\n",
    "# Use the relative path name for the model and other files \n",
    "# (CoreNLP is two levels up from the current working directory)\n",
    "STANFORD = os.path.join(\"../..\", \"stanford-corenlp-4.5.1/\")\n",
    "\n",
    "# Create the server\n",
    "# The server needs to know the location of the following files:\n",
    "#   - stanford-corenlp-X.X.X.jar\n",
    "#   - stanford-corenlp-X.X.X-models.jar\n",
    "\n",
    "server = CoreNLPServer(\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-4.5.1.jar\"),\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-4.5.1-models.jar\"),    \n",
    ")\n",
    "\n",
    "# Start the server in the background\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server in the background (if dead)\n",
    "# server.start()\n",
    "\n",
    "from  nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser()\n",
    "parse = next(parser.raw_parse(\"John ate the cake with spoon\"))\n",
    "print(parse)\n",
    "\n",
    "# Stop the server when done\n",
    "# server.stop()\n",
    "\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analysis\n",
    "We see only one tree. This tree has various grammar rules that you have learned during the class activities and almost complete. The resultant tree works for \"John ate the cake with spoon\". \n",
    "\n",
    "Try the same with code with \"John ate the cake with cherry\".\n",
    "The tree generated will be incorrect.\n",
    "\n",
    "Hence we need the semantics that can help us to disambiguate the words ate, cake, cherry, spoon. This aspect is left to you as a Homework (see the slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "Write the code to draw the parse tree for the second sentence: \"After she ate the cake, Emma visited Tony in his room\"  \n",
    "It is way too easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server in the background again\n",
    "# server.start()\n",
    "\n",
    "## Enter your code below\n",
    "parse =\n",
    "\n",
    "# Stop the server when done\n",
    "# server.stop()\n",
    "\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Discourse analysis\n",
    "We will now uncover the discourse structure of a text.\n",
    "\n",
    "I love data science because I find it very useful for my company.\n",
    "\n",
    "To achieve this:\n",
    "1. Generate the parse tree for the given  text from stanford parser.\n",
    "2. Call the StanfordDependencyParser to generate dependencies. List of dependencies are generated. look out for \"mark\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server in the background again\n",
    "# server.start()\n",
    "\n",
    "sentences = next(parser.raw_parse(\"I love data science because I find it very useful for my company.\"))\n",
    "print(sentences)\n",
    "# sentences.draw()\n",
    "\n",
    "# Stop the server when done\n",
    "# server.stop()\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parser\n",
    "\n",
    "The Stanford typed dependencies representation was designed to provide a simple description of the\n",
    "grammatical relationships in a sentence that can easily be understood and effectively used by people\n",
    "without linguistic expertise who want to extract textual relations. I\n",
    "\n",
    "The details of the representation are available in the manual. \n",
    "https://nlp.stanford.edu/software/dependencies_manual.pdf\n",
    "\n",
    "More information about the universal dependencies can be found in https://nlp.stanford.edu/pubs/schuster2016enhanced.pdf\n",
    "\n",
    "Let us continue to work with our example and generate the dependencies using our CoreNLP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server in the background again\n",
    "# server.start()\n",
    "\n",
    "from  nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "dependency_parser = CoreNLPDependencyParser()\n",
    "result = next(dependency_parser.raw_parse('I love data science because I find it very useful for my company.'))\n",
    "\n",
    "# Stop the server when done\n",
    "# server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "You will need to install `graphviz` to visualize the `result` variable, which is a dependency graph.\n",
    "- Open an Anaconda prompt on Windows. Or a terminal on Mac.\n",
    "- Type in `conda install graphviz`\n",
    "- You may need `pip install graphviz` as well. (Do this `pip` step only if the next cell fails.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Windows, you need to add GraphViz to the path as well\n",
    "if (myOS == \"Windows\"):\n",
    "    username = 'lenovo'\n",
    "    os.environ[\"PATH\"] += os.pathsep + f'''C:\\\\Users\\\\{username}\\Anaconda3\\Library\\bin\\graphviz'''\n",
    "    \n",
    "# Visualize the result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Discourse\n",
    "\n",
    "<b>mark: marker </b>\n",
    "\n",
    "A marker is the word introducing a finite clause subordinate to another clause. For a complement clause,\n",
    "this will typically be “that” or “whether”. For an adverbial clause, the marker is typically a preposition\n",
    "like “while” or “although”. The mark is a dependent of the subordinate clause head.\n",
    "\n",
    "https://nlp.stanford.edu/pubs/schuster2016enhanced.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    " Write the code to generate tags for the text \"After she ate the cake, Emma visited Tony in his room\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "### Loading our own dataset - SGNews\n",
    "\n",
    "The `data.zip` file downloaded from eLearn contains two folders: `SampleText` and `SGNews_Apr2012`. \n",
    "This `SGNews_Apr2012` data\n",
    "set contains a set of Singapore news articles in April 2012. Load this\n",
    "document collection using NLTK. Can you find out the following\n",
    "information of this collection?\n",
    "-   Number of documents in the collection.\n",
    "-   Total number of words in the collection.\n",
    "-   The top-20 most frequent words in the file, 14011.txt.\n",
    "### Tips:\n",
    "\n",
    "-   You can use NLTK or gensim\n",
    "-   To use `FreqDist`, you can either use `nltk.FreqDist()` as shown\n",
    "    above or type `from nltk.probability import FreqDist` first and then directly use\n",
    "    `FreqDist()`. This is because the `FreqDist` class is defined by the\n",
    "    `probability` module under NLTK.\n",
    "-   You may use `counter` from the gensim package as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Enter your code here to answer the questions above. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
